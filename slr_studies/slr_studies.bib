
@misc{zhang_aflow_2025,
	title = {{AFlow}: {Automating} {Agentic} {Workflow} {Generation}},
	shorttitle = {{AFlow}},
	url = {http://arxiv.org/abs/2410.10762},
	doi = {10.48550/arXiv.2410.10762},
	abstract = {Large language models (LLMs) have demonstrated remarkable potential in solving complex tasks across diverse domains, typically by employing agentic workflows that follow detailed instructions and operational sequences. However, constructing these workflows requires significant human effort, limiting scalability and generalizability. Recent research has sought to automate the generation and optimization of these workflows, but existing methods still rely on initial manual setup and fall short of achieving fully automated and effective workflow generation. To address this challenge, we reformulate workflow optimization as a search problem over code-represented workflows, where LLM-invoking nodes are connected by edges. We introduce AFlow, an automated framework that efficiently explores this space using Monte Carlo Tree Search, iteratively refining workflows through code modification, tree-structured experience, and execution feedback. Empirical evaluations across six benchmark datasets demonstrate AFlow's efficacy, yielding a 5.7\% average improvement over state-of-the-art baselines. Furthermore, AFlow enables smaller models to outperform GPT-4o on specific tasks at 4.55\% of its inference cost in dollars. The code is available at https://github.com/FoundationAgents/AFlow.},
	urldate = {2025-12-31},
	publisher = {arXiv},
	author = {Zhang, Jiayi and Xiang, Jinyu and Yu, Zhaoyang and Teng, Fengwei and Chen, Xionghui and Chen, Jiaqi and Zhuge, Mingchen and Cheng, Xin and Hong, Sirui and Wang, Jinlin and Zheng, Bingnan and Liu, Bang and Luo, Yuyu and Wu, Chenglin},
	month = apr,
	year = {2025},
	note = {arXiv:2410.10762 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:files/876/Zhang et al. - 2025 - AFlow Automating Agentic Workflow Generation.pdf:application/pdf;Snapshot:files/877/2410.html:text/html},
}

@misc{niu_flow_2025,
	title = {Flow: {Modularized} {Agentic} {Workflow} {Automation}},
	shorttitle = {Flow},
	url = {http://arxiv.org/abs/2501.07834},
	doi = {10.48550/arXiv.2501.07834},
	abstract = {Multi-agent frameworks powered by large language models (LLMs) have demonstrated great success in automated planning and task execution. However, the effective adjustment of agentic workflows during execution has not been well studied. An effective workflow adjustment is crucial in real-world scenarios, as the initial plan must adjust to unforeseen challenges and changing conditions in real time to ensure the efficient execution of complex tasks. In this paper, we define workflows as an activity-on-vertex (AOV) graph, which allows continuous workflow refinement by LLM agents through dynamic subtask allocation adjustment based on historical performance and previous AOVs. To further enhance framework performance, we emphasize modularity in workflow design based on evaluating parallelism and dependency complexity. With this design, our proposed multi-agent framework achieves efficient concurrent execution of subtasks, effective goal achievement, and enhanced error tolerance. Empirical results across various practical tasks demonstrate significant improvements in the efficiency of multi-agent frameworks through dynamic workflow refinement and modularization. The code is available at: https://github.com/tmllab/2025\_ICLR\_FLOW.},
	urldate = {2025-12-31},
	publisher = {arXiv},
	author = {Niu, Boye and Song, Yiliao and Lian, Kai and Shen, Yifan and Yao, Yu and Zhang, Kun and Liu, Tongliang},
	month = feb,
	year = {2025},
	note = {arXiv:2501.07834 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems},
	file = {Preprint PDF:files/879/Niu et al. - 2025 - Flow Modularized Agentic Workflow Automation.pdf:application/pdf;Snapshot:files/880/2501.html:text/html},
}

@misc{ehtesham_survey_2025,
	title = {A survey of agent interoperability protocols: {Model} {Context} {Protocol} ({MCP}), {Agent} {Communication} {Protocol} ({ACP}), {Agent}-to-{Agent} {Protocol} ({A2A}), and {Agent} {Network} {Protocol} ({ANP})},
	shorttitle = {A survey of agent interoperability protocols},
	url = {http://arxiv.org/abs/2505.02279},
	doi = {10.48550/arXiv.2505.02279},
	abstract = {Large language model powered autonomous agents demand robust, standardized protocols to integrate tools, share contextual data, and coordinate tasks across heterogeneous systems. Ad-hoc integrations are difficult to scale, secure, and generalize across domains. This survey examines four emerging agent communication protocols: Model Context Protocol (MCP), Agent Communication Protocol (ACP), Agent-to-Agent Protocol (A2A), and Agent Network Protocol (ANP), each addressing interoperability in deployment contexts. MCP provides a JSON-RPC client-server interface for secure tool invocation and typed data exchange. ACP defines a general-purpose communication protocol over RESTful HTTP, supporting MIME-typed multipart messages and synchronous and asynchronous interactions. Its lightweight and runtime-independent design enables scalable agent invocation, while features like session management, message routing, and integration with role-based and decentralized identifiers (DIDs). A2A enables peer-to-peer task delegation using capability-based Agent Cards, supporting secure and scalable collaboration across enterprise agent workflows. ANP supports open network agent discovery and secure collaboration using W3C decentralized identifiers DIDs and JSON-LD graphs. The protocols are compared across multiple dimensions, including interaction modes, discovery mechanisms, communication patterns, and security models. Based on the comparative analysis, a phased adoption roadmap is proposed: beginning with MCP for tool access, followed by ACP for structured, multimodal messaging session-aware interaction and both online and offline agent discovery across scalable, HTTP-based deployments A2A for collaborative task execution, and extending to ANP for decentralized agent marketplaces. This work provides a comprehensive foundation for designing secure, interoperable, and scalable ecosystems of LLM-powered agents.},
	urldate = {2025-12-31},
	publisher = {arXiv},
	author = {Ehtesham, Abul and Singh, Aditi and Gupta, Gaurav Kumar and Kumar, Saket},
	month = may,
	year = {2025},
	note = {arXiv:2505.02279 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:files/882/Ehtesham et al. - 2025 - A survey of agent interoperability protocols Model Context Protocol (MCP), Agent Communication Prot.pdf:application/pdf;Snapshot:files/883/2505.html:text/html},
}

@misc{hu_automated_2025,
	title = {Automated {Design} of {Agentic} {Systems}},
	url = {http://arxiv.org/abs/2408.08435},
	doi = {10.48550/arXiv.2408.08435},
	abstract = {Researchers are investing substantial effort in developing powerful general-purpose agents, wherein Foundation Models are used as modules within agentic systems (e.g. Chain-of-Thought, Self-Reflection, Toolformer). However, the history of machine learning teaches us that hand-designed solutions are eventually replaced by learned solutions. We describe a newly forming research area, Automated Design of Agentic Systems (ADAS), which aims to automatically create powerful agentic system designs, including inventing novel building blocks and/or combining them in new ways. We further demonstrate that there is an unexplored yet promising approach within ADAS where agents can be defined in code and new agents can be automatically discovered by a meta agent programming ever better ones in code. Given that programming languages are Turing Complete, this approach theoretically enables the learning of any possible agentic system: including novel prompts, tool use, workflows, and combinations thereof. We present a simple yet effective algorithm named Meta Agent Search to demonstrate this idea, where a meta agent iteratively programs interesting new agents based on an ever-growing archive of previous discoveries. Through extensive experiments across multiple domains including coding, science, and math, we show that our algorithm can progressively invent agents with novel designs that greatly outperform state-of-the-art hand-designed agents. Importantly, we consistently observe the surprising result that agents invented by Meta Agent Search maintain superior performance even when transferred across domains and models, demonstrating their robustness and generality. Provided we develop it safely, our work illustrates the potential of an exciting new research direction toward automatically designing ever-more powerful agentic systems to benefit humanity.},
	urldate = {2025-12-31},
	publisher = {arXiv},
	author = {Hu, Shengran and Lu, Cong and Clune, Jeff},
	month = mar,
	year = {2025},
	note = {arXiv:2408.08435 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:files/886/Hu et al. - 2025 - Automated Design of Agentic Systems.pdf:application/pdf;Snapshot:files/887/2408.html:text/html},
}

@misc{li_camel_2023,
	title = {{CAMEL}: {Communicative} {Agents} for "{Mind}" {Exploration} of {Large} {Language} {Model} {Society}},
	shorttitle = {{CAMEL}},
	url = {http://arxiv.org/abs/2303.17760},
	doi = {10.48550/arXiv.2303.17760},
	abstract = {The rapid advancement of chat-based language models has led to remarkable progress in complex task-solving. However, their success heavily relies on human input to guide the conversation, which can be challenging and time-consuming. This paper explores the potential of building scalable techniques to facilitate autonomous cooperation among communicative agents, and provides insight into their "cognitive" processes. To address the challenges of achieving autonomous cooperation, we propose a novel communicative agent framework named role-playing. Our approach involves using inception prompting to guide chat agents toward task completion while maintaining consistency with human intentions. We showcase how role-playing can be used to generate conversational data for studying the behaviors and capabilities of a society of agents, providing a valuable resource for investigating conversational language models. In particular, we conduct comprehensive studies on instruction-following cooperation in multi-agent settings. Our contributions include introducing a novel communicative agent framework, offering a scalable approach for studying the cooperative behaviors and capabilities of multi-agent systems, and open-sourcing our library to support research on communicative agents and beyond: https://github.com/camel-ai/camel.},
	urldate = {2025-12-31},
	publisher = {arXiv},
	author = {Li, Guohao and Hammoud, Hasan Abed Al Kader and Itani, Hani and Khizbullin, Dmitrii and Ghanem, Bernard},
	month = nov,
	year = {2023},
	note = {arXiv:2303.17760 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Multiagent Systems, Computer Science - Computers and Society},
	file = {Preprint PDF:files/890/Li et al. - 2023 - CAMEL Communicative Agents for Mind Exploration of Large Language Model Society.pdf:application/pdf;Snapshot:files/891/2303.html:text/html},
}

@misc{shen_hugginggpt_2023,
	title = {{HuggingGPT}: {Solving} {AI} {Tasks} with {ChatGPT} and its {Friends} in {Hugging} {Face}},
	shorttitle = {{HuggingGPT}},
	url = {http://arxiv.org/abs/2303.17580},
	doi = {10.48550/arXiv.2303.17580},
	abstract = {Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.},
	urldate = {2025-12-31},
	publisher = {arXiv},
	author = {Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting},
	month = dec,
	year = {2023},
	note = {arXiv:2303.17580 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:files/893/Shen et al. - 2023 - HuggingGPT Solving AI Tasks with ChatGPT and its Friends in Hugging Face.pdf:application/pdf;Snapshot:files/894/2303.html:text/html},
}

@misc{schick_toolformer_2023,
	title = {Toolformer: {Language} {Models} {Can} {Teach} {Themselves} to {Use} {Tools}},
	shorttitle = {Toolformer},
	url = {http://arxiv.org/abs/2302.04761},
	doi = {10.48550/arXiv.2302.04761},
	abstract = {Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q{\textbackslash}\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.},
	urldate = {2025-12-31},
	publisher = {arXiv},
	author = {Schick, Timo and Dwivedi-Yu, Jane and Dessì, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
	month = feb,
	year = {2023},
	note = {arXiv:2302.04761 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:files/896/Schick et al. - 2023 - Toolformer Language Models Can Teach Themselves to Use Tools.pdf:application/pdf;Snapshot:files/897/2302.html:text/html},
}

@misc{han_mdocagent_2025,
	title = {{MDocAgent}: {A} {Multi}-{Modal} {Multi}-{Agent} {Framework} for {Document} {Understanding}},
	shorttitle = {{MDocAgent}},
	url = {http://arxiv.org/abs/2503.13964},
	doi = {10.48550/arXiv.2503.13964},
	abstract = {Document Question Answering (DocQA) is a very common task. Existing methods using Large Language Models (LLMs) or Large Vision Language Models (LVLMs) and Retrieval Augmented Generation (RAG) often prioritize information from a single modal, failing to effectively integrate textual and visual cues. These approaches struggle with complex multi-modal reasoning, limiting their performance on real-world documents. We present MDocAgent (A Multi-Modal Multi-Agent Framework for Document Understanding), a novel RAG and multi-agent framework that leverages both text and image. Our system employs five specialized agents: a general agent, a critical agent, a text agent, an image agent and a summarizing agent. These agents engage in multi-modal context retrieval, combining their individual insights to achieve a more comprehensive understanding of the document's content. This collaborative approach enables the system to synthesize information from both textual and visual components, leading to improved accuracy in question answering. Preliminary experiments on five benchmarks like MMLongBench, LongDocURL demonstrate the effectiveness of our MDocAgent, achieve an average improvement of 12.1\% compared to current state-of-the-art method. This work contributes to the development of more robust and comprehensive DocQA systems capable of handling the complexities of real-world documents containing rich textual and visual information. Our data and code are available at https://github.com/aiming-lab/MDocAgent.},
	urldate = {2025-12-31},
	publisher = {arXiv},
	author = {Han, Siwei and Xia, Peng and Zhang, Ruiyi and Sun, Tong and Li, Yun and Zhu, Hongtu and Yao, Huaxiu},
	month = mar,
	year = {2025},
	note = {arXiv:2503.13964 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:files/901/Han et al. - 2025 - MDocAgent A Multi-Modal Multi-Agent Framework for Document Understanding.pdf:application/pdf;Snapshot:files/902/2503.html:text/html},
}

@misc{qian_chatdev_2024,
	title = {{ChatDev}: {Communicative} {Agents} for {Software} {Development}},
	shorttitle = {{ChatDev}},
	url = {http://arxiv.org/abs/2307.07924},
	doi = {10.48550/arXiv.2307.07924},
	abstract = {Software development is a complex task that necessitates cooperation among multiple members with diverse skills. Numerous studies used deep learning to improve specific phases in a waterfall model, such as design, coding, and testing. However, the deep learning model in each phase requires unique designs, leading to technical inconsistencies across various phases, which results in a fragmented and ineffective development process. In this paper, we introduce ChatDev, a chat-powered software development framework in which specialized agents driven by large language models (LLMs) are guided in what to communicate (via chat chain) and how to communicate (via communicative dehallucination). These agents actively contribute to the design, coding, and testing phases through unified language-based communication, with solutions derived from their multi-turn dialogues. We found their utilization of natural language is advantageous for system design, and communicating in programming language proves helpful in debugging. This paradigm demonstrates how linguistic communication facilitates multi-agent collaboration, establishing language as a unifying bridge for autonomous task-solving among LLM agents. The code and data are available at https://github.com/OpenBMB/ChatDev.},
	urldate = {2025-12-31},
	publisher = {arXiv},
	author = {Qian, Chen and Liu, Wei and Liu, Hongzhang and Chen, Nuo and Dang, Yufan and Li, Jiahao and Yang, Cheng and Chen, Weize and Su, Yusheng and Cong, Xin and Xu, Juyuan and Li, Dahai and Liu, Zhiyuan and Sun, Maosong},
	month = jun,
	year = {2024},
	note = {arXiv:2307.07924 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Computation and Language, Computer Science - Multiagent Systems},
	file = {Preprint PDF:files/905/Qian et al. - 2024 - ChatDev Communicative Agents for Software Development.pdf:application/pdf;Snapshot:files/906/2307.html:text/html},
}

@misc{wu_autogen_2023,
	title = {{AutoGen}: {Enabling} {Next}-{Gen} {LLM} {Applications} via {Multi}-{Agent} {Conversation}},
	shorttitle = {{AutoGen}},
	url = {http://arxiv.org/abs/2308.08155},
	doi = {10.48550/arXiv.2308.08155},
	abstract = {AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.},
	urldate = {2025-12-31},
	publisher = {arXiv},
	author = {Wu, Qingyun and Bansal, Gagan and Zhang, Jieyu and Wu, Yiran and Li, Beibin and Zhu, Erkang and Jiang, Li and Zhang, Xiaoyun and Zhang, Shaokun and Liu, Jiale and Awadallah, Ahmed Hassan and White, Ryen W. and Burger, Doug and Wang, Chi},
	month = oct,
	year = {2023},
	note = {arXiv:2308.08155 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:files/909/Wu et al. - 2023 - AutoGen Enabling Next-Gen LLM Applications via Multi-Agent Conversation.pdf:application/pdf;Snapshot:files/910/2308.html:text/html},
}

@misc{hong_metagpt_2024,
	title = {{MetaGPT}: {Meta} {Programming} for {A} {Multi}-{Agent} {Collaborative} {Framework}},
	shorttitle = {{MetaGPT}},
	url = {http://arxiv.org/abs/2308.00352},
	doi = {10.48550/arXiv.2308.00352},
	abstract = {Remarkable progress has been made on automated problem solving through societies of agents based on large language models (LLMs). Existing LLM-based multi-agent systems can already solve simple dialogue tasks. Solutions to more complex tasks, however, are complicated through logic inconsistencies due to cascading hallucinations caused by naively chaining LLMs. Here we introduce MetaGPT, an innovative meta-programming framework incorporating efficient human workflows into LLM-based multi-agent collaborations. MetaGPT encodes Standardized Operating Procedures (SOPs) into prompt sequences for more streamlined workflows, thus allowing agents with human-like domain expertise to verify intermediate results and reduce errors. MetaGPT utilizes an assembly line paradigm to assign diverse roles to various agents, efficiently breaking down complex tasks into subtasks involving many agents working together. On collaborative software engineering benchmarks, MetaGPT generates more coherent solutions than previous chat-based multi-agent systems. Our project can be found at https://github.com/geekan/MetaGPT},
	urldate = {2025-12-31},
	publisher = {arXiv},
	author = {Hong, Sirui and Zhuge, Mingchen and Chen, Jiaqi and Zheng, Xiawu and Cheng, Yuheng and Zhang, Ceyao and Wang, Jinlin and Wang, Zili and Yau, Steven Ka Shing and Lin, Zijuan and Zhou, Liyang and Ran, Chenyu and Xiao, Lingfeng and Wu, Chenglin and Schmidhuber, Jürgen},
	month = nov,
	year = {2024},
	note = {arXiv:2308.00352 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems},
	file = {Preprint PDF:files/912/Hong et al. - 2024 - MetaGPT Meta Programming for A Multi-Agent Collaborative Framework.pdf:application/pdf;Snapshot:files/913/2308.html:text/html},
}

@misc{shinn_reflexion_2023,
	title = {Reflexion: {Language} {Agents} with {Verbal} {Reinforcement} {Learning}},
	shorttitle = {Reflexion},
	url = {http://arxiv.org/abs/2303.11366},
	doi = {10.48550/arXiv.2303.11366},
	abstract = {Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback. Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. Reflexion is flexible enough to incorporate various types (scalar values or free-form language) and sources (external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks (sequential decision-making, coding, language reasoning). For example, Reflexion achieves a 91\% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves 80\%. We also conduct ablation and analysis studies using different feedback signals, feedback incorporation methods, and agent types, and provide insights into how they affect performance.},
	urldate = {2025-12-31},
	publisher = {arXiv},
	author = {Shinn, Noah and Cassano, Federico and Berman, Edward and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
	month = oct,
	year = {2023},
	note = {arXiv:2303.11366 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:files/916/Shinn et al. - 2023 - Reflexion Language Agents with Verbal Reinforcement Learning.pdf:application/pdf;Snapshot:files/917/2303.html:text/html},
}

@misc{yao_react_2023,
	title = {{ReAct}: {Synergizing} {Reasoning} and {Acting} in {Language} {Models}},
	shorttitle = {{ReAct}},
	url = {http://arxiv.org/abs/2210.03629},
	doi = {10.48550/arXiv.2210.03629},
	abstract = {While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34\% and 10\% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io},
	urldate = {2025-12-31},
	publisher = {arXiv},
	author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
	month = mar,
	year = {2023},
	note = {arXiv:2210.03629 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:files/920/Yao et al. - 2023 - ReAct Synergizing Reasoning and Acting in Language Models.pdf:application/pdf;Snapshot:files/921/2210.html:text/html},
}

@article{kitchenham_guidelines_2007,
	title = {Guidelines for performing {Systematic} {Literature} {Reviews} in {Software} {Engineering}},
	volume = {2},
	abstract = {The objective of this report is to propose comprehensive guidelines for systematic literature reviews appropriate for software engineering researchers, including PhD students. A systematic literature review is a means of evaluating and interpreting all available research relevant to a particular research question, topic area, or phenomenon of interest. 
Systematic reviews aim to present a fair evaluation of a research topic by using a trustworthy, rigorous, and auditable methodology. The guidelines presented in this report were derived from three existing guidelines used by medical researchers, two books produced by researchers with social science backgrounds and discussions with researchers from other disciplines who are involved
in evidence-based practice. The guidelines have been adapted to reflect the specific problems of software engineering research.
The guidelines cover three phases of a systematic literature review: planning the review, conducting the review and reporting the review. They provide a relatively high level description. They do not consider the impact of the research questions on the review procedures, nor do they specify in detail the mechanisms needed to perform meta-analysis.},
	author = {Kitchenham, Barbara and Charters, Stuart},
	month = jan,
	year = {2007},
	file = {Full Text PDF:files/926/Kitchenham und Charters - 2007 - Guidelines for performing Systematic Literature Reviews in Software Engineering.pdf:application/pdf},
}

@article{ali_agentic_2025,
	title = {Agentic {AI}: {A} {Comprehensive} {Survey} of {Architectures}, {Applications}, and {Future} {Directions}},
	volume = {59},
	issn = {1573-7462},
	shorttitle = {Agentic {AI}},
	url = {http://arxiv.org/abs/2510.25445},
	doi = {10.1007/s10462-025-11422-4},
	abstract = {Agentic AI represents a transformative shift in artificial intelligence, but its rapid advancement has led to a fragmented understanding, often conflating modern neural systems with outdated symbolic models -- a practice known as conceptual retrofitting. This survey cuts through this confusion by introducing a novel dual-paradigm framework that categorizes agentic systems into two distinct lineages: the Symbolic/Classical (relying on algorithmic planning and persistent state) and the Neural/Generative (leveraging stochastic generation and prompt-driven orchestration). Through a systematic PRISMA-based review of 90 studies (2018--2025), we provide a comprehensive analysis structured around this framework across three dimensions: (1) the theoretical foundations and architectural principles defining each paradigm; (2) domain-specific implementations in healthcare, finance, and robotics, demonstrating how application constraints dictate paradigm selection; and (3) paradigm-specific ethical and governance challenges, revealing divergent risks and mitigation strategies. Our analysis reveals that the choice of paradigm is strategic: symbolic systems dominate safety-critical domains (e.g., healthcare), while neural systems prevail in adaptive, data-rich environments (e.g., finance). Furthermore, we identify critical research gaps, including a significant deficit in governance models for symbolic systems and a pressing need for hybrid neuro-symbolic architectures. The findings culminate in a strategic roadmap arguing that the future of Agentic AI lies not in the dominance of one paradigm, but in their intentional integration to create systems that are both adaptable and reliable. This work provides the essential conceptual toolkit to guide future research, development, and policy toward robust and trustworthy hybrid intelligent systems.},
	number = {1},
	urldate = {2025-12-31},
	journal = {Artif Intell Rev},
	author = {Ali, Mohamad Abou and Dornaika, Fadi},
	month = nov,
	year = {2025},
	note = {arXiv:2510.25445 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	pages = {11},
	file = {Preprint PDF:files/929/Ali und Dornaika - 2025 - Agentic AI A Comprehensive Survey of Architectures, Applications, and Future Directions.pdf:application/pdf;Snapshot:files/928/2510.html:text/html},
}

@misc{asai_self-rag_2023,
	title = {Self-{RAG}: {Learning} to {Retrieve}, {Generate}, and {Critique} through {Self}-{Reflection}},
	shorttitle = {Self-{RAG}},
	url = {http://arxiv.org/abs/2310.11511},
	doi = {10.48550/arXiv.2310.11511},
	abstract = {Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models.},
	urldate = {2025-12-31},
	publisher = {arXiv},
	author = {Asai, Akari and Wu, Zeqiu and Wang, Yizhong and Sil, Avirup and Hajishirzi, Hannaneh},
	month = oct,
	year = {2023},
	note = {arXiv:2310.11511 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:files/932/Asai et al. - 2023 - Self-RAG Learning to Retrieve, Generate, and Critique through Self-Reflection.pdf:application/pdf;Snapshot:files/933/2310.html:text/html},
}

@misc{zeng_routine_2025,
	title = {Routine: {A} {Structural} {Planning} {Framework} for {LLM} {Agent} {System} in {Enterprise}},
	shorttitle = {Routine},
	url = {http://arxiv.org/abs/2507.14447},
	doi = {10.48550/arXiv.2507.14447},
	abstract = {The deployment of agent systems in an enterprise environment is often hindered by several challenges: common models lack domain-specific process knowledge, leading to disorganized plans, missing key tools, and poor execution stability. To address this, this paper introduces Routine, a multi-step agent planning framework designed with a clear structure, explicit instructions, and seamless parameter passing to guide the agent's execution module in performing multi-step tool-calling tasks with high stability. In evaluations conducted within a real-world enterprise scenario, Routine significantly increases the execution accuracy in model tool calls, increasing the performance of GPT-4o from 41.1\% to 96.3\%, and Qwen3-14B from 32.6\% to 83.3\%. We further constructed a Routine-following training dataset and fine-tuned Qwen3-14B, resulting in an accuracy increase to 88.2\% on scenario-specific evaluations, indicating improved adherence to execution plans. In addition, we employed Routine-based distillation to create a scenario-specific, multi-step tool-calling dataset. Fine-tuning on this distilled dataset raised the model's accuracy to 95.5\%, approaching GPT-4o's performance. These results highlight Routine's effectiveness in distilling domain-specific tool-usage patterns and enhancing model adaptability to new scenarios. Our experimental results demonstrate that Routine provides a practical and accessible approach to building stable agent workflows, accelerating the deployment and adoption of agent systems in enterprise environments, and advancing the technical vision of AI for Process.},
	urldate = {2025-12-31},
	publisher = {arXiv},
	author = {Zeng, Guancheng and Chen, Xueyi and Hu, Jiawang and Qi, Shaohua and Mao, Yaxuan and Wang, Zhantao and Nie, Yifan and Li, Shuang and Feng, Qiuyang and Qiu, Pengxu and Wang, Yujia and Han, Wenqiang and Huang, Linyan and Li, Gang and Mo, Jingjing and Hu, Haowen},
	month = jul,
	year = {2025},
	note = {arXiv:2507.14447 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:files/936/Zeng et al. - 2025 - Routine A Structural Planning Framework for LLM Agent System in Enterprise.pdf:application/pdf;Snapshot:files/937/2507.html:text/html},
}

@misc{xi_rise_2023,
	title = {The {Rise} and {Potential} of {Large} {Language} {Model} {Based} {Agents}: {A} {Survey}},
	shorttitle = {The {Rise} and {Potential} of {Large} {Language} {Model} {Based} {Agents}},
	url = {http://arxiv.org/abs/2309.07864},
	doi = {10.48550/arXiv.2309.07864},
	abstract = {For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level, with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. Many efforts have been made to develop intelligent agents, but they mainly focus on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually, what the community lacks is a general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios. Due to the versatile capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many researchers have leveraged LLMs as the foundation to build AI agents and have achieved significant progress. In this paper, we perform a comprehensive survey on LLM-based agents. We start by tracing the concept of agents from its philosophical origins to its development in AI, and explain why LLMs are suitable foundations for agents. Building upon this, we present a general framework for LLM-based agents, comprising three main components: brain, perception, and action, and the framework can be tailored for different applications. Subsequently, we explore the extensive applications of LLM-based agents in three aspects: single-agent scenarios, multi-agent scenarios, and human-agent cooperation. Following this, we delve into agent societies, exploring the behavior and personality of LLM-based agents, the social phenomena that emerge from an agent society, and the insights they offer for human society. Finally, we discuss several key topics and open problems within the field. A repository for the related papers at https://github.com/WooooDyy/LLM-Agent-Paper-List.},
	urldate = {2025-12-31},
	publisher = {arXiv},
	author = {Xi, Zhiheng and Chen, Wenxiang and Guo, Xin and He, Wei and Ding, Yiwen and Hong, Boyang and Zhang, Ming and Wang, Junzhe and Jin, Senjie and Zhou, Enyu and Zheng, Rui and Fan, Xiaoran and Wang, Xiao and Xiong, Limao and Zhou, Yuhao and Wang, Weiran and Jiang, Changhao and Zou, Yicheng and Liu, Xiangyang and Yin, Zhangyue and Dou, Shihan and Weng, Rongxiang and Cheng, Wensen and Zhang, Qi and Qin, Wenjuan and Zheng, Yongyan and Qiu, Xipeng and Huang, Xuanjing and Gui, Tao},
	month = sep,
	year = {2023},
	note = {arXiv:2309.07864 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:files/941/Xi et al. - 2023 - The Rise and Potential of Large Language Model Based Agents A Survey.pdf:application/pdf;Snapshot:files/940/2309.html:text/html},
}

@article{wang_survey_2024,
	title = {A {Survey} on {Large} {Language} {Model} based {Autonomous} {Agents}},
	volume = {18},
	issn = {2095-2228, 2095-2236},
	url = {http://arxiv.org/abs/2308.11432},
	doi = {10.1007/s11704-024-40231-1},
	abstract = {Autonomous agents have long been a prominent research focus in both academic and industry communities. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of LLM-based autonomous agents from a holistic perspective. More specifically, we first discuss the construction of LLM-based autonomous agents, for which we propose a unified framework that encompasses a majority of the previous work. Then, we present a comprehensive overview of the diverse applications of LLM-based autonomous agents in the fields of social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field. To keep track of this field and continuously update our survey, we maintain a repository of relevant references at https://github.com/Paitesanshi/LLM-Agent-Survey.},
	number = {6},
	urldate = {2025-12-31},
	journal = {Front. Comput. Sci.},
	author = {Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and Zhao, Wayne Xin and Wei, Zhewei and Wen, Ji-Rong},
	month = dec,
	year = {2024},
	note = {arXiv:2308.11432 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	pages = {186345},
	file = {Preprint PDF:files/945/Wang et al. - 2024 - A Survey on Large Language Model based Autonomous Agents.pdf:application/pdf;Snapshot:files/944/2308.html:text/html},
}

@misc{cai_lightagent_2025,
	title = {{LightAgent}: {Production}-level {Open}-source {Agentic} {AI} {Framework}},
	shorttitle = {{LightAgent}},
	url = {http://arxiv.org/abs/2509.09292},
	doi = {10.48550/arXiv.2509.09292},
	abstract = {With the rapid advancement of large language models (LLMs), Multi-agent Systems (MAS) have achieved significant progress in various application scenarios. However, substantial challenges remain in designing versatile, robust, and efficient platforms for agent deployment. To address these limitations, we propose {\textbackslash}textbf\{LightAgent\}, a lightweight yet powerful agentic framework, effectively resolving the trade-off between flexibility and simplicity found in existing frameworks. LightAgent integrates core functionalities such as Memory (mem0), Tools, and Tree of Thought (ToT), while maintaining an extremely lightweight structure. As a fully open-source solution, it seamlessly integrates with mainstream chat platforms, enabling developers to easily build self-learning agents. We have released LightAgent at {\textbackslash}href\{https://github.com/wxai-space/LightAgent\}\{https://github.com/wxai-space/LightAgent\}},
	urldate = {2025-12-31},
	publisher = {arXiv},
	author = {Cai, Weige and Zhu, Tong and Niu, Jinyi and Hu, Ruiqi and Li, Lingyao and Wang, Tenglong and Dai, Xiaowu and Shen, Weining and Zhang, Liwen},
	month = sep,
	year = {2025},
	note = {arXiv:2509.09292 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:files/947/Cai et al. - 2025 - LightAgent Production-level Open-source Agentic AI Framework.pdf:application/pdf;Snapshot:files/948/2509.html:text/html},
}

@misc{sami_adaptive_2025,
	title = {Adaptive {Multi}-{Agent} {Reasoning} via {Automated} {Workflow} {Generation}},
	url = {http://arxiv.org/abs/2507.14393},
	doi = {10.48550/arXiv.2507.14393},
	abstract = {The rise of Large Reasoning Models (LRMs) promises a significant leap forward in language model capabilities, aiming to tackle increasingly sophisticated tasks with unprecedented efficiency and accuracy. However, despite their impressive performance, recent studies have highlighted how current reasoning models frequently fail to generalize to novel, unseen problems, often resorting to memorized solutions rather than genuine inferential reasoning. Such behavior underscores a critical limitation in modern LRMs, i.e., their tendency toward overfitting, which in turn results in poor generalization in problem-solving capabilities. In this paper, we introduce Nexus Architect, an enhanced iteration of our multi-agent system framework, Nexus, equipped with a novel automated workflow synthesis mechanism. Given a user's prompt and a small set of representative examples, the Architect autonomously generates a tailored reasoning workflow by selecting suitable strategies, tool integrations, and adversarial techniques for a specific problem class. Furthermore, the Architect includes an iterative prompt refinement mechanism that fine-tunes agents' system prompts to maximize performance and improve the generalization capabilities of the system. We empirically evaluate Nexus Architect by employing an off-the-shelf, non-reasoning model on a custom dataset of challenging logical questions and compare its performance against state-of-the-art LRMs. Results show that Nexus Architect consistently outperforms existing solutions, achieving up to a 66\% increase in pass rate over Gemini 2.5 Flash Preview, nearly 2.5\${\textbackslash}times\$ against Claude Sonnet 4 and DeepSeek-R1, and over 3\${\textbackslash}times\$ w.r.t. Llama 4 Scout.},
	urldate = {2025-12-31},
	publisher = {arXiv},
	author = {Sami, Humza and Islam, Mubashir ul and Gaillardon, Pierre-Emmanuel and Tenace, Valerio},
	month = jul,
	year = {2025},
	note = {arXiv:2507.14393 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:files/950/Sami et al. - 2025 - Adaptive Multi-Agent Reasoning via Automated Workflow Generation.pdf:application/pdf;Snapshot:files/951/2507.html:text/html},
}

@misc{singh_agentic_2025,
	title = {Agentic {Retrieval}-{Augmented} {Generation}: {A} {Survey} on {Agentic} {RAG}},
	shorttitle = {Agentic {Retrieval}-{Augmented} {Generation}},
	url = {http://arxiv.org/abs/2501.09136},
	doi = {10.48550/arXiv.2501.09136},
	abstract = {Large Language Models (LLMs) have revolutionized artificial intelligence (AI) by enabling human like text generation and natural language understanding. However, their reliance on static training data limits their ability to respond to dynamic, real time queries, resulting in outdated or inaccurate outputs. Retrieval Augmented Generation (RAG) has emerged as a solution, enhancing LLMs by integrating real time data retrieval to provide contextually relevant and up-to-date responses. Despite its promise, traditional RAG systems are constrained by static workflows and lack the adaptability required for multistep reasoning and complex task management. Agentic Retrieval-Augmented Generation (Agentic RAG) transcends these limitations by embedding autonomous AI agents into the RAG pipeline. These agents leverage agentic design patterns reflection, planning, tool use, and multiagent collaboration to dynamically manage retrieval strategies, iteratively refine contextual understanding, and adapt workflows to meet complex task requirements. This integration enables Agentic RAG systems to deliver unparalleled flexibility, scalability, and context awareness across diverse applications. This survey provides a comprehensive exploration of Agentic RAG, beginning with its foundational principles and the evolution of RAG paradigms. It presents a detailed taxonomy of Agentic RAG architectures, highlights key applications in industries such as healthcare, finance, and education, and examines practical implementation strategies. Additionally, it addresses challenges in scaling these systems, ensuring ethical decision making, and optimizing performance for real-world applications, while providing detailed insights into frameworks and tools for implementing Agentic RAG.},
	urldate = {2025-12-31},
	publisher = {arXiv},
	author = {Singh, Aditi and Ehtesham, Abul and Kumar, Saket and Khoei, Tala Talaei},
	month = feb,
	year = {2025},
	note = {arXiv:2501.09136 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {Preprint PDF:files/953/Singh et al. - 2025 - Agentic Retrieval-Augmented Generation A Survey on Agentic RAG.pdf:application/pdf;Snapshot:files/954/2501.html:text/html},
}

@misc{bandara_practical_2025,
	title = {A {Practical} {Guide} for {Designing}, {Developing}, and {Deploying} {Production}-{Grade} {Agentic} {AI} {Workflows}},
	url = {http://arxiv.org/abs/2512.08769},
	doi = {10.48550/arXiv.2512.08769},
	abstract = {Agentic AI marks a major shift in how autonomous systems reason, plan, and execute multi-step tasks. Unlike traditional single model prompting, agentic workflows integrate multiple specialized agents with different Large Language Models(LLMs), tool-augmented capabilities, orchestration logic, and external system interactions to form dynamic pipelines capable of autonomous decision-making and action. As adoption accelerates across industry and research, organizations face a central challenge: how to design, engineer, and operate production-grade agentic AI workflows that are reliable, observable, maintainable, and aligned with safety and governance requirements. This paper provides a practical, end-to-end guide for designing, developing, and deploying production-quality agentic AI systems. We introduce a structured engineering lifecycle encompassing workflow decomposition, multi-agent design patterns, Model Context Protocol(MCP), and tool integration, deterministic orchestration, Responsible-AI considerations, and environment-aware deployment strategies. We then present nine core best practices for engineering production-grade agentic AI workflows, including tool-first design over MCP, pure-function invocation, single-tool and single-responsibility agents, externalized prompt management, Responsible-AI-aligned model-consortium design, clean separation between workflow logic and MCP servers, containerized deployment for scalable operations, and adherence to the Keep it Simple, Stupid (KISS) principle to maintain simplicity and robustness. To demonstrate these principles in practice, we present a comprehensive case study: a multimodal news-analysis and media-generation workflow. By combining architectural guidance, operational patterns, and practical implementation insights, this paper offers a foundational reference to build robust, extensible, and production-ready agentic AI workflows.},
	urldate = {2025-12-31},
	publisher = {arXiv},
	author = {Bandara, Eranga and Gore, Ross and Foytik, Peter and Shetty, Sachin and Mukkamala, Ravi and Rahman, Abdul and Liang, Xueping and Bouk, Safdar H. and Hass, Amin and Rajapakse, Sachini and Keong, Ng Wee and Zoysa, Kasun De and Withanage, Aruna and Loganathan, Nilaan},
	month = dec,
	year = {2025},
	note = {arXiv:2512.08769 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:files/956/Bandara et al. - 2025 - A Practical Guide for Designing, Developing, and Deploying Production-Grade Agentic AI Workflows.pdf:application/pdf;Snapshot:files/957/2512.html:text/html},
}

@book{wohlin_experimentation_2012,
	address = {Berlin, Heidelberg},
	edition = {1},
	title = {Experimentation in {Software} {Engineering}},
	isbn = {978-3-642-29044-2},
	url = {https://link.springer.com/book/10.1007/978-3-642-29044-2},
	abstract = {Like other sciences and engineering disciplines, software engineering requires a cycle of model building, experimentation, and learning. Experiments are valuable tools for all software engineers who are involved in evaluating and choosing between different methods, techniques, languages and tools. The purpose of Experimentation in Software Engineering is to introduce students, teachers, researchers, and practitioners to empirical studies in software engineering, using controlled experiments. The introduction to experimentation is provided through a process perspective, and the focus is on the steps that we have to go through to perform an experiment. The book is divided into three parts. The first part provides a background of theories and methods used in experimentation. Part II then devotes one chapter to each of the five experiment steps: scoping, planning, execution, analysis, and result presentation. Part III completes the presentation with two examples. Assignments and statistical material are provided in appendixes. Overall the book provides indispensable information regarding empirical studies in particular for experiments, but also for case studies, systematic literature reviews, and surveys. It is a revision of the authors’ book, which was published in 2000. In addition, substantial new material, e.g. concerning systematic literature reviews and case study research, is introduced. The book is self-contained and it is suitable as a course book in undergraduate or graduate studies where the need for empirical studies in software engineering is stressed. Exercises and assignments are included to combine the more theoretical material with practical aspects. Researchers will also benefit from the book, learning more about how to conduct empirical studies, and likewise practitioners may use it as a “cookbook” when evaluating new methods or techniques before implementing them in their organization.},
	language = {en},
	urldate = {2026-01-11},
	publisher = {Springer Berlin Heidelberg},
	author = {Wohlin, Claes and Runeson, Per and Höst, Martin and Ohlsson, Magnus C. and Regnell, Björn and Wesslén, Anders},
	month = jun,
	year = {2012},
	file = {PDF:files/964/Wohlin et al. - 2012 - Experimentation in Software Engineering.pdf:application/pdf;Snapshot:files/959/978-3-642-29044-2.html:text/html},
}

@article{carrera-rivera_how-conduct_2022,
	title = {How-to conduct a systematic literature review: {A} quick guide for computer science research},
	volume = {9},
	issn = {2215-0161},
	shorttitle = {How-to conduct a systematic literature review},
	url = {https://www.sciencedirect.com/science/article/pii/S2215016122002746},
	doi = {10.1016/j.mex.2022.101895},
	abstract = {Performing a literature review is a critical first step in research to understanding the state-of-the-art and identifying gaps and challenges in the field. A systematic literature review is a method which sets out a series of steps to methodically organize the review. In this paper, we present a guide designed for researchers and in particular early-stage researchers in the computer-science field. The contribution of the article is the following:•Clearly defined strategies to follow for a systematic literature review in computer science research, and•Algorithmic method to tackle a systematic literature review.},
	urldate = {2026-01-11},
	journal = {MethodsX},
	author = {Carrera-Rivera, Angela and Ochoa, William and Larrinaga, Felix and Lasa, Ganix},
	month = jan,
	year = {2022},
	keywords = {computer science, doctoral studies, literature reviews, research methodology, Systematic literature reviews},
	pages = {101895},
	file = {ScienceDirect Snapshot:files/961/S2215016122002746.html:text/html;Volltext:files/962/Carrera-Rivera et al. - 2022 - How-to conduct a systematic literature review A quick guide for computer science research.pdf:application/pdf},
}

@article{gusenbauer_which_2020,
	title = {Which academic search systems are suitable for systematic reviews or meta-analyses? {Evaluating} retrieval qualities of {Google} {Scholar}, {PubMed}, and 26 other resources},
	volume = {11},
	copyright = {© 2019 The Authors. Research Synthesis Methods published by John Wiley \& Sons Ltd},
	issn = {1759-2887},
	shorttitle = {Which academic search systems are suitable for systematic reviews or meta-analyses?},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.1378},
	doi = {10.1002/jrsm.1378},
	abstract = {Rigorous evidence identification is essential for systematic reviews and meta-analyses (evidence syntheses) because the sample selection of relevant studies determines a review's outcome, validity, and explanatory power. Yet, the search systems allowing access to this evidence provide varying levels of precision, recall, and reproducibility and also demand different levels of effort. To date, it remains unclear which search systems are most appropriate for evidence synthesis and why. Advice on which search engines and bibliographic databases to choose for systematic searches is limited and lacking systematic, empirical performance assessments. This study investigates and compares the systematic search qualities of 28 widely used academic search systems, including Google Scholar, PubMed, and Web of Science. A novel, query-based method tests how well users are able to interact and retrieve records with each system. The study is the first to show the extent to which search systems can effectively and efficiently perform (Boolean) searches with regards to precision, recall, and reproducibility. We found substantial differences in the performance of search systems, meaning that their usability in systematic searches varies. Indeed, only half of the search systems analyzed and only a few Open Access databases can be recommended for evidence syntheses without adding substantial caveats. Particularly, our findings demonstrate why Google Scholar is inappropriate as principal search system. We call for database owners to recognize the requirements of evidence synthesis and for academic journals to reassess quality requirements for systematic reviews. Our findings aim to support researchers in conducting better searches for better evidence synthesis.},
	language = {en},
	number = {2},
	urldate = {2026-01-11},
	journal = {Research Synthesis Methods},
	author = {Gusenbauer, Michael and Haddaway, Neal R.},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jrsm.1378},
	keywords = {academic search systems, discovery, evaluation, information retrieval, systematic review, systematic search},
	pages = {181--217},
	file = {Full Text PDF:files/966/Gusenbauer und Haddaway - 2020 - Which academic search systems are suitable for systematic reviews or meta-analyses Evaluating retri.pdf:application/pdf;Snapshot:files/967/jrsm.html:text/html},
}
